# Session 2: Building with LLMs - RAG Implementation

## Welcome to Session 2!

Today we'll build on what you learned in Session 1 and implement **RAG (Retrieval-Augmented Generation)** - one of the most powerful and immediately valuable AI techniques for businesses. RAG allows AI to answer questions based on your specific documents and knowledge, not just general training data.

## What You'll Build

By the end of this session, you'll have a working RAG system that can:
- Load and process local documents (PDFs, text files)
- Answer questions based on your specific content
- Provide accurate, contextual responses for business use cases

## Jordan Business Applications

RAG is perfect for Jordanian businesses because it can:
- **Customer Service**: Answer questions from product manuals or FAQs
- **Document Processing**: Analyze contracts, reports, or legal documents
- **Knowledge Management**: Make company policies and procedures searchable
- **Content Creation**: Generate responses based on your specific brand guidelines

## Prerequisites

- **Session 1 completed** (you know how to make LLM API calls)
- **Python environment** set up from Session 1
- **Sample documents** (we'll provide some, or use your own)

## Step 1: Understanding RAG

Before we code, let's understand how RAG works:

### The Problem with Basic LLM Calls
```python
# Basic LLM call (from Session 1)
response = call_llm("ÙƒÙŠÙ Ø£ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø¹Ù…ÙŠÙ„ ØºØ§Ø¶Ø¨ØŸ")
# Response: General advice, may not fit your business context
```

### The RAG Solution
```python
# RAG approach
documents = ["customer_service_guide.txt", "company_policy.txt"]
response = rag_system.query("ÙƒÙŠÙ Ø£ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø¹Ù…ÙŠÙ„ ØºØ§Ø¶Ø¨ØŸ", documents)
# Response: Specific advice based on YOUR documents
```

### How RAG Works
1. **Load Documents**: Read your files (PDFs, text, etc.)
2. **Chunk Text**: Break documents into smaller pieces
3. **Create Embeddings**: Convert text to numerical vectors
4. **Find Similar Content**: Match user questions to relevant chunks
5. **Generate Response**: Use similar chunks as context for LLM

## Step 2: Setting Up RAG Components

First, let's create the basic RAG structure:

```bash
# Create Session 2 directory
mkdir -p code/session-2
code code/session-2/simple_rag.py
```

## Step 3: Basic RAG Implementation

Copy this code to start building your RAG system:

```python
#!/usr/bin/env python3
"""
Session 2: Simple RAG Implementation
Building a basic RAG system for document-based Q&A
"""

import os
import openai
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import PyPDF2
import re

load_dotenv()

class SimpleRAG:
    """
    A simple RAG system for document-based Q&A
    """

    def __init__(self):
        """Initialize the RAG system"""
        print("ğŸ”§ Initializing RAG system...")
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
        self.documents = []  # Store document chunks
        self.embeddings = []  # Store embeddings
        self.metadata = []   # Store metadata for each chunk
        print("âœ… RAG system ready!")

    def chunk_text(self, text, chunk_size=500, overlap=50):
        """
        Split text into overlapping chunks

        Args:
            text: The text to chunk
            chunk_size: Maximum characters per chunk
            overlap: Characters to overlap between chunks
        """
        chunks = []

        # Simple chunking by character count
        for i in range(0, len(text), chunk_size - overlap):
            chunk = text[i:i + chunk_size]
            if len(chunk.strip()) > 100:  # Only keep substantial chunks
                chunks.append(chunk.strip())

        return chunks

    def add_document(self, text, metadata=None):
        """
        Add a document to the RAG system

        Args:
            text: Document content
            metadata: Additional information about the document
        """
        print(f"ğŸ“„ Processing document ({len(text)} characters)...")

        # Chunk the text
        chunks = self.chunk_text(text)

        # Create embeddings for each chunk
        for chunk in chunks:
            chunk_embedding = self.embedder.encode(chunk)
            self.documents.append(chunk)
            self.embeddings.append(chunk_embedding)
            self.metadata.append(metadata or {})

        print(f"âœ… Added {len(chunks)} chunks from document")

    def load_text_file(self, file_path):
        """Load a text file"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            return content
        except Exception as e:
            print(f"âŒ Error loading {file_path}: {e}")
            return None

    def load_pdf_file(self, file_path):
        """Load a PDF file"""
        try:
            with open(file_path, 'rb') as f:
                pdf_reader = PyPDF2.PdfReader(f)
                content = ""
                for page in pdf_reader.pages:
                    content += page.extract_text() + "\n"
            return content
        except Exception as e:
            print(f"âŒ Error loading PDF {file_path}: {e}")
            return None

    def find_similar_chunks(self, query, top_k=3):
        """
        Find the most similar document chunks to a query

        Args:
            query: User question
            top_k: Number of chunks to return
        """
        # Encode the query
        query_embedding = self.embedder.encode(query)

        # Calculate similarities
        similarities = cosine_similarity(
            [query_embedding], self.embeddings
        )[0]

        # Get indices of most similar chunks
        top_indices = np.argsort(similarities)[-top_k:][::-1]

        return [(self.documents[i], self.metadata[i], similarities[i])
                for i in top_indices]

    def query(self, question, top_k=3):
        """
        Answer a question using RAG

        Args:
            question: User's question
            top_k: Number of relevant chunks to use
        """
        print(f"ğŸ¤” Answering: {question}")

        # Find similar chunks
        similar_chunks = self.find_similar_chunks(question, top_k)

        if not similar_chunks:
            return "âŒ No relevant information found in documents"

        # Build context from similar chunks
        context = "\n\n".join([chunk for chunk, _, _ in similar_chunks])

        # Create the prompt for LLM
        prompt = f"""
Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©ØŒ Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:

Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©:
{context}

Ø§Ù„Ø³Ø¤Ø§Ù„: {question}

ÙŠØ±Ø¬Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø© Ù…ÙÙŠØ¯Ø© ÙˆØ¯Ù‚ÙŠÙ‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙ‚Ø·.
"""

        # Call the LLM
        try:
            client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù…ÙÙŠØ¯ ÙŠØ¬ÙŠØ¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© ÙÙ‚Ø·."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,  # Lower temperature for factual responses
                max_tokens=300
            )

            answer = response.choices[0].message.content

            # Add source information
            sources = [metadata.get('source', 'Unknown') for _, metadata, _ in similar_chunks]
            answer += f"\n\nğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø±: {', '.join(set(sources))}"

            return answer

        except Exception as e:
            return f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ: {str(e)}"

# Test the RAG system
if __name__ == "__main__":
    # Initialize RAG system
    rag = SimpleRAG()

    # Add a sample document (you can replace this with your own files)
    sample_text = """
Ø¯Ù„ÙŠÙ„ Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ - Ø§Ù„Ø£Ø±Ø¯Ù†

ÙƒÙŠÙÙŠØ© Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø§Ù„ØºØ§Ø¶Ø¨ÙŠÙ†:
1. Ø§Ø³ØªÙ…Ø¹ Ø¨Ø¹Ù†Ø§ÙŠØ© Ù„Ø´ÙƒÙˆÙ‰ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¯ÙˆÙ† Ù…Ù‚Ø§Ø·Ø¹Ø©
2. Ø§Ø¹ØªØ°Ø± Ø¹Ù† Ø£ÙŠ Ø¥Ø²Ø¹Ø§Ø¬ ØªØ³Ø¨Ø¨Ù†Ø§ ÙÙŠÙ‡
3. Ø§Ø³Ø£Ù„ Ø£Ø³Ø¦Ù„Ø© Ù„ÙÙ‡Ù… Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø¨ÙˆØ¶ÙˆØ­
4. Ù‚Ø¯Ù… Ø­Ù„ÙˆÙ„ Ø¹Ù…Ù„ÙŠØ© ÙˆØ³Ø±ÙŠØ¹Ø©
5. ØªØ§Ø¨Ø¹ Ù…Ø¹ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø±Ø¶Ø§Ù‡

Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø¥Ø±Ø¬Ø§Ø¹:
- ÙŠÙ…ÙƒÙ† Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø®Ù„Ø§Ù„ 14 ÙŠÙˆÙ… Ù…Ù† Ø§Ù„Ø´Ø±Ø§Ø¡
- ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª ÙÙŠ Ø­Ø§Ù„ØªÙ‡Ø§ Ø§Ù„Ø£ØµÙ„ÙŠØ©
- Ø³ÙŠØªÙ… Ø§Ø³ØªØ±Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø¨Ù„Øº Ø®Ù„Ø§Ù„ 3-5 Ø£ÙŠØ§Ù… Ø¹Ù…Ù„

Ø®Ø¯Ù…Ø© Ø§Ù„ØªÙˆØµÙŠÙ„:
- ØªÙˆØµÙŠÙ„ Ù…Ø¬Ø§Ù†ÙŠ Ù„Ù„Ø·Ù„Ø¨Ø§Øª ÙÙˆÙ‚ 50 Ø¯ÙŠÙ†Ø§Ø±
- Ù†ÙØ³ Ø§Ù„ÙŠÙˆÙ… ÙÙŠ Ø¹Ù…Ø§Ù† ÙˆØ§Ù„Ø²Ø±Ù‚Ø§Ø¡
- 1-2 ÙŠÙˆÙ… Ø¹Ù…Ù„ Ù„Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø§Øª
"""

    rag.add_document(sample_text, {"source": "customer_service_guide.txt"})

    # Test queries
    test_questions = [
        "ÙƒÙŠÙ Ø£ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø¹Ù…ÙŠÙ„ ØºØ§Ø¶Ø¨ØŸ",
        "Ù…Ø§ Ù‡ÙŠ Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø¥Ø±Ø¬Ø§Ø¹ØŸ",
        "ÙƒÙ… ÙŠØ³ØªØºØ±Ù‚ Ø§Ù„ØªÙˆØµÙŠÙ„ ÙÙŠ Ø¥Ø±Ø¨Ø¯ØŸ"
    ]

    for question in test_questions:
        print("\n" + "="*60)
        answer = rag.query(question)
        print(f"â“ Ø§Ù„Ø³Ø¤Ø§Ù„: {question}")
        print(f"ğŸ’¡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {answer}")
        print("="*60)
```

## Step 4: Testing with Your Own Documents

Let's create a sample business document for testing:

```bash
# Create sample documents
cat > sample_documents/customer_service_guide.txt << 'EOF'
Ø¯Ù„ÙŠÙ„ Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ - Ù…ØªØ¬Ø± Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ø£Ø±Ø¯Ù†ÙŠ

Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø§Ù„ØºØ§Ø¶Ø¨ÙŠÙ†:
1. Ø§Ø¨Ø¯Ø£ Ø¨Ø§Ù„ØªØ±Ø­ÙŠØ¨ ÙˆØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø§Ø¹ØªØ°Ø§Ø± Ø§Ù„ÙÙˆØ±ÙŠ
2. Ø§Ø³ØªÙ…Ø¹ Ù„Ù„Ù…Ø´ÙƒÙ„Ø© ÙƒØ§Ù…Ù„Ø© Ø¯ÙˆÙ† Ù…Ù‚Ø§Ø·Ø¹Ø©
3. Ø£Ø¹Ø¯ ØµÙŠØ§ØºØ© Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„ÙÙ‡Ù…
4. Ù‚Ø¯Ù… Ø­Ù„ÙˆÙ„ Ù…ØªØ¹Ø¯Ø¯Ø© Ù„Ù„Ø¹Ù…ÙŠÙ„
5. Ø§Ø®ØªØ± Ø§Ù„Ø­Ù„ Ø§Ù„Ø£Ù†Ø³Ø¨ Ù…Ø¹ Ø§Ù„Ø¹Ù…ÙŠÙ„
6. ØªØ§Ø¨Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø®Ù„Ø§Ù„ 24 Ø³Ø§Ø¹Ø©

Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø¥Ø±Ø¬Ø§Ø¹ ÙˆØ§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„:
- Ø¥Ø±Ø¬Ø§Ø¹ Ù…Ø¬Ø§Ù†ÙŠ Ø®Ù„Ø§Ù„ 30 ÙŠÙˆÙ… Ù…Ù† Ø§Ù„Ø´Ø±Ø§Ø¡
- Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† ÙÙŠ Ø§Ù„Ø¹Ø¨ÙˆØ© Ø§Ù„Ø£ØµÙ„ÙŠØ©
- Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª: Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ù…Ø®ØµØµØ© ÙˆØ§Ù„ÙƒØªØ¨ Ø§Ù„Ù…Ø³ØªØ¹Ù…Ù„Ø©
- Ø§Ø³ØªØ±Ø¯Ø§Ø¯ Ø®Ù„Ø§Ù„ 5-7 Ø£ÙŠØ§Ù… Ø¹Ù…Ù„

Ø®Ø¯Ù…Ø© Ø§Ù„ØªÙˆØµÙŠÙ„:
- Ù…Ø¬Ø§Ù†ÙŠ Ù„Ù„Ø·Ù„Ø¨Ø§Øª ÙÙˆÙ‚ 25 Ø¯ÙŠÙ†Ø§Ø±
- Ù†ÙØ³ Ø§Ù„ÙŠÙˆÙ… ÙÙŠ Ø¹Ù…Ø§Ù† (Ù‚Ø¨Ù„ Ø§Ù„Ø³Ø§Ø¹Ø© 2 Ø¸Ù‡Ø±Ø§Ù‹)
- 1 ÙŠÙˆÙ… Ø¹Ù…Ù„ ÙÙŠ Ø§Ù„Ø²Ø±Ù‚Ø§Ø¡ ÙˆÙ…Ø§Ø¯Ø¨Ø§
- 2 ÙŠÙˆÙ… Ø¹Ù…Ù„ ÙÙŠ Ø¥Ø±Ø¨Ø¯ ÙˆØ§Ù„ÙƒØ±Ùƒ
- 3 Ø£ÙŠØ§Ù… Ø¹Ù…Ù„ Ù„Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø§Øª
EOF

cat > sample_documents/company_faq.txt << 'EOF'
Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© - Ù…ØªØ¬Ø±Ù†Ø§ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ

Ø³: ÙƒÙŠÙ Ø£ØªØ¨Ø¹ Ø·Ù„Ø¨ÙŠØŸ
Ø¬: Ø§Ø³ØªØ®Ø¯Ù… Ø±Ù‚Ù… Ø§Ù„Ø·Ù„Ø¨ ÙÙŠ ØµÙØ­Ø© "ØªØªØ¨Ø¹ Ø§Ù„Ø·Ù„Ø¨" Ø¹Ù„Ù‰ Ù…ÙˆÙ‚Ø¹Ù†Ø§

Ø³: Ù‡Ù„ ØªÙ‚Ø¨Ù„ÙˆÙ† Ø§Ù„Ø¯ÙØ¹ Ø¹Ù†Ø¯ Ø§Ù„ØªØ³Ù„ÙŠÙ…ØŸ
Ø¬: Ù†Ø¹Ù…ØŒ Ù†Ù‚Ø¨Ù„ Ø§Ù„Ø¯ÙØ¹ Ø§Ù„Ù†Ù‚Ø¯ÙŠ Ø¹Ù†Ø¯ Ø§Ù„ØªØ³Ù„ÙŠÙ… ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¯Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©

Ø³: Ù…Ø§ Ù‡ÙŠ Ø·Ø±Ù‚ Ø§Ù„Ø¯ÙØ¹ Ø§Ù„Ù…ØªØ§Ø­Ø©ØŸ
Ø¬: Ù†Ù‚Ø¨Ù„ ÙÙŠØ²Ø§ØŒ Ù…Ø§Ø³ØªØ±ÙƒØ§Ø±Ø¯ØŒ ÙˆØ§Ù„Ø¯ÙØ¹ Ø¹Ù†Ø¯ Ø§Ù„ØªØ³Ù„ÙŠÙ…

Ø³: Ù‡Ù„ Ù„Ø¯ÙŠÙƒÙ… Ø¶Ù…Ø§Ù† Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†ØªØ¬Ø§ØªØŸ
Ø¬: Ù†Ø¹Ù…ØŒ Ø¶Ù…Ø§Ù† Ù„Ù…Ø¯Ø© Ø¹Ø§Ù… Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ©

Ø³: ÙƒÙŠÙ Ø£ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ØŸ
Ø¬: Ù‡Ø§ØªÙ: 0791234567
ÙˆØ§ØªØ³Ø§Ø¨: 0777654321
Ø¨Ø±ÙŠØ¯ Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ: support@store.jo
Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø¹Ù…Ù„: 9 ØµØ¨Ø§Ø­Ø§Ù‹ - 9 Ù…Ø³Ø§Ø¡Ù‹ ÙŠÙˆÙ…ÙŠØ§Ù‹
EOF
```

## Step 5: Advanced RAG Features

Let's enhance your RAG system with more features:

```python
#!/usr/bin/env python3
"""
Advanced RAG Features
"""

# ... existing code ...

def add_multiple_documents(self, file_paths):
    """Add multiple documents at once"""
    for file_path in file_paths:
        if file_path.endswith('.txt'):
            content = self.load_text_file(file_path)
        elif file_path.endswith('.pdf'):
            content = self.load_pdf_file(file_path)
        else:
            print(f"âš ï¸  Unsupported file type: {file_path}")
            continue

        if content:
            metadata = {"source": file_path}
            self.add_document(content, metadata)

def query_with_confidence(self, question, top_k=3, threshold=0.3):
    """Query with confidence scoring"""
    similar_chunks = self.find_similar_chunks(question, top_k)

    # Filter by similarity threshold
    confident_chunks = [(chunk, meta, sim) for chunk, meta, sim in similar_chunks if sim > threshold]

    if not confident_chunks:
        return "âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„"

    # Rest of the query logic...
```

## Step 6: Integration with Your Existing Systems

RAG can enhance your current PHP/.NET applications:

```python
# Example: Add RAG to existing customer service system
def integrate_with_php_system(rag_system, question):
    """
    Example of how to integrate RAG with PHP/.NET backend
    """
    # Get answer from RAG
    answer = rag_system.query(question)

    # Format for your existing system
    response = {
        "answer": answer,
        "confidence": "high",  # You can calculate this
        "sources": ["customer_guide", "faq"],
        "timestamp": "2025-10-18"
    }

    return response

# Use in your PHP backend:
# $rag_answer = call_rag_api($question);
// return $rag_answer to frontend
```

## You're Ready for Session 2! ğŸ‰

You now have:
- âœ… Basic RAG system implementation
- âœ… Document loading and processing
- âœ… Similarity search and context building
- âœ… LLM integration with document context
- âœ… Sample documents for testing
- âœ… Ideas for business integration

## Next Steps for the Workshop

1. **Test with your own documents** (company policies, product manuals, etc.)
2. **Experiment with different chunk sizes** and see how they affect results
3. **Try different types of questions** and observe the quality of responses
4. **Think about integration points** with your existing systems

See you in the workshop where we'll dive deeper into these concepts and build more advanced features!
