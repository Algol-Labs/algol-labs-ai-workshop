#!/usr/bin/env python3
"""
Session 2: Simple RAG Implementation
Building a basic RAG system for document-based Q&A
"""

import os
import openai
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import PyPDF2
import re

load_dotenv()

class SimpleRAG:
    """
    A simple RAG system for document-based Q&A
    """

    def __init__(self):
        """Initialize the RAG system"""
        print("ğŸ”§ Initializing RAG system...")
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
        self.documents = []  # Store document chunks
        self.embeddings = []  # Store embeddings
        self.metadata = []   # Store metadata for each chunk
        print("âœ… RAG system ready!")

    def chunk_text(self, text, chunk_size=500, overlap=50):
        """
        Split text into overlapping chunks

        Args:
            text: The text to chunk
            chunk_size: Maximum characters per chunk
            overlap: Characters to overlap between chunks
        """
        chunks = []

        # Simple chunking by character count
        for i in range(0, len(text), chunk_size - overlap):
            chunk = text[i:i + chunk_size]
            if len(chunk.strip()) > 100:  # Only keep substantial chunks
                chunks.append(chunk.strip())

        return chunks

    def add_document(self, text, metadata=None):
        """
        Add a document to the RAG system

        Args:
            text: Document content
            metadata: Additional information about the document
        """
        print(f"ğŸ“„ Processing document ({len(text)} characters)...")

        # Chunk the text
        chunks = self.chunk_text(text)

        # Create embeddings for each chunk
        for chunk in chunks:
            chunk_embedding = self.embedder.encode(chunk)
            self.documents.append(chunk)
            self.embeddings.append(chunk_embedding)
            self.metadata.append(metadata or {})

        print(f"âœ… Added {len(chunks)} chunks from document")

    def load_text_file(self, file_path):
        """Load a text file"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            return content
        except Exception as e:
            print(f"âŒ Error loading {file_path}: {e}")
            return None

    def load_pdf_file(self, file_path):
        """Load a PDF file"""
        try:
            with open(file_path, 'rb') as f:
                pdf_reader = PyPDF2.PdfReader(f)
                content = ""
                for page in pdf_reader.pages:
                    content += page.extract_text() + "\n"
            return content
        except Exception as e:
            print(f"âŒ Error loading PDF {file_path}: {e}")
            return None

    def find_similar_chunks(self, query, top_k=3):
        """
        Find the most similar document chunks to a query

        Args:
            query: User question
            top_k: Number of chunks to return
        """
        # Encode the query
        query_embedding = self.embedder.encode(query)

        # Calculate similarities
        similarities = cosine_similarity(
            [query_embedding], self.embeddings
        )[0]

        # Get indices of most similar chunks
        top_indices = np.argsort(similarities)[-top_k:][::-1]

        return [(self.documents[i], self.metadata[i], similarities[i])
                for i in top_indices]

    def query(self, question, top_k=3):
        """
        Answer a question using RAG

        Args:
            question: User's question
            top_k: Number of relevant chunks to use
        """
        print(f"ğŸ¤” Answering: {question}")

        # Find similar chunks
        similar_chunks = self.find_similar_chunks(question, top_k)

        if not similar_chunks:
            return "âŒ No relevant information found in documents"

        # Build context from similar chunks
        context = "\n\n".join([chunk for chunk, _, _ in similar_chunks])

        # Create the prompt for LLM
        prompt = f"""
Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©ØŒ Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:

Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©:
{context}

Ø§Ù„Ø³Ø¤Ø§Ù„: {question}

ÙŠØ±Ø¬Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø© Ù…ÙÙŠØ¯Ø© ÙˆØ¯Ù‚ÙŠÙ‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙ‚Ø·.
"""

        # Call the LLM
        try:
            client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù…ÙÙŠØ¯ ÙŠØ¬ÙŠØ¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© ÙÙ‚Ø·."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,  # Lower temperature for factual responses
                max_tokens=300
            )

            answer = response.choices[0].message.content

            # Add source information
            sources = [metadata.get('source', 'Unknown') for _, metadata, _ in similar_chunks]
            answer += f"\n\nğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø±: {', '.join(set(sources))}"

            return answer

        except Exception as e:
            return f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ: {str(e)}"

# Test the RAG system
if __name__ == "__main__":
    # Initialize RAG system
    rag = SimpleRAG()

    # Add a sample document (you can replace this with your own files)
    sample_text = """
Ø¯Ù„ÙŠÙ„ Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ - Ø§Ù„Ø£Ø±Ø¯Ù†

ÙƒÙŠÙÙŠØ© Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø§Ù„ØºØ§Ø¶Ø¨ÙŠÙ†:
1. Ø§Ø³ØªÙ…Ø¹ Ø¨Ø¹Ù†Ø§ÙŠØ© Ù„Ø´ÙƒÙˆÙ‰ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¯ÙˆÙ† Ù…Ù‚Ø§Ø·Ø¹Ø©
2. Ø§Ø¹ØªØ°Ø± Ø¹Ù† Ø£ÙŠ Ø¥Ø²Ø¹Ø§Ø¬ ØªØ³Ø¨Ø¨Ù†Ø§ ÙÙŠÙ‡
3. Ø§Ø³Ø£Ù„ Ø£Ø³Ø¦Ù„Ø© Ù„ÙÙ‡Ù… Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø¨ÙˆØ¶ÙˆØ­
4. Ù‚Ø¯Ù… Ø­Ù„ÙˆÙ„ Ø¹Ù…Ù„ÙŠØ© ÙˆØ³Ø±ÙŠØ¹Ø©
5. ØªØ§Ø¨Ø¹ Ù…Ø¹ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø±Ø¶Ø§Ù‡

Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø¥Ø±Ø¬Ø§Ø¹:
- ÙŠÙ…ÙƒÙ† Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø®Ù„Ø§Ù„ 14 ÙŠÙˆÙ… Ù…Ù† Ø§Ù„Ø´Ø±Ø§Ø¡
- ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª ÙÙŠ Ø­Ø§Ù„ØªÙ‡Ø§ Ø§Ù„Ø£ØµÙ„ÙŠØ©
- Ø³ÙŠØªÙ… Ø§Ø³ØªØ±Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø¨Ù„Øº Ø®Ù„Ø§Ù„ 3-5 Ø£ÙŠØ§Ù… Ø¹Ù…Ù„

Ø®Ø¯Ù…Ø© Ø§Ù„ØªÙˆØµÙŠÙ„:
- ØªÙˆØµÙŠÙ„ Ù…Ø¬Ø§Ù†ÙŠ Ù„Ù„Ø·Ù„Ø¨Ø§Øª ÙÙˆÙ‚ 50 Ø¯ÙŠÙ†Ø§Ø±
- Ù†ÙØ³ Ø§Ù„ÙŠÙˆÙ… ÙÙŠ Ø¹Ù…Ø§Ù† ÙˆØ§Ù„Ø²Ø±Ù‚Ø§Ø¡
- 1-2 ÙŠÙˆÙ… Ø¹Ù…Ù„ Ù„Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø§Øª
"""

    rag.add_document(sample_text, {"source": "customer_service_guide.txt"})

    # Test queries
    test_questions = [
        "ÙƒÙŠÙ Ø£ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø¹Ù…ÙŠÙ„ ØºØ§Ø¶Ø¨ØŸ",
        "Ù…Ø§ Ù‡ÙŠ Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø¥Ø±Ø¬Ø§Ø¹ØŸ",
        "ÙƒÙ… ÙŠØ³ØªØºØ±Ù‚ Ø§Ù„ØªÙˆØµÙŠÙ„ ÙÙŠ Ø¥Ø±Ø¨Ø¯ØŸ"
    ]

    for question in test_questions:
        print("\n" + "="*60)
        answer = rag.query(question)
        print(f"â“ Ø§Ù„Ø³Ø¤Ø§Ù„: {question}")
        print(f"ğŸ’¡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {answer}")
        print("="*60)
